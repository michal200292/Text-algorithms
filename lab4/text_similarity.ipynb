{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Metrics\n",
    "\n",
    "Exercise notebook\n",
    "\n",
    "Course: Algorytmy Tekstowe at AGH University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and vectorization\n",
    "\n",
    "1. Preprocessing: Convert the text documents to lowercase and remove all punctuation marks (using regular expressions, for example).\n",
    "2. Vocabulary creation: Create a vocabulary by taking all unique words from all text documents.\n",
    "3. Word frequency vectors: Create two vectors, each representing the frequency of each word in the vocabulary in each text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    text = \"\".join([c for c in text if c not in punctuation])\n",
    "    return text.lower()\n",
    "\n",
    "def text_to_vec(docs: list[str]) -> list[list[int]]:\n",
    "\n",
    "    docs = list(map(lambda vec: preprocess(vec), docs))\n",
    "    docs = [doc.split(\" \") for doc in docs]\n",
    "    alphabet = {}\n",
    "\n",
    "    ind = 0\n",
    "    for key in set([word for doc in docs for word in doc]):\n",
    "        alphabet[key] = ind\n",
    "        ind += 1\n",
    "\n",
    "    freq_vecs = []\n",
    "    for doc in docs:\n",
    "        vector = [0 for _ in range(ind)]\n",
    "        for word in doc:\n",
    "            vector[alphabet[word]] += 1\n",
    "        freq_vecs.append(vector)\n",
    "    return freq_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "text_a = \"The quick brown fox jumped over the lazy dog.\"\n",
    "text_b = \"The lazy dog was jumped over by the quick brown fox.\"\n",
    "vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "\n",
    "assert(set(vec_a) == {1, 1, 1, 2, 1, 1, 1, 1, 0, 0})\n",
    "assert(set(vec_b) == {1, 1, 1, 2, 1, 1, 1, 1, 1, 1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}= \\frac{\\sum\\limits_{i=1}^{n} A_i B_i}{\\sqrt{\\sum\\limits_{i=1}^{n} A_i^2} \\sqrt{\\sum\\limits_{i=1}^{n} B_i^2}}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &\\mathbf{A}\\text{ and }\\mathbf{B} \\text{ are the two vectors being compared}\\\\\n",
    "    &n \\text{ is the dimensionality of the vectors}\\\\\n",
    "    &\\theta \\text{ represents the angle between two vectors } \\mathbf{A} \\text{ and } \\mathbf{B} \\text{ in a high-dimensional space}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The dot product of $\\mathbf{A}$ and $\\mathbf{B}$ is divided by the product of their Euclidean lengths to normalize the result to a range of [-1, 1]. A value of 1 indicates that the two vectors are identical, while a value of -1 indicates that they are completely dissimilar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(text_a: str, text_b: str) -> float:\n",
    "    vec_1, vec_2 = text_to_vec([text_a, text_b])\n",
    "    vec_1 = np.array(vec_1)\n",
    "    vec_2 = np.array(vec_2)\n",
    "    \n",
    "    return vec_1.dot(vec_2) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = cosine_similarity(text_a, text_b)\n",
    "assert(abs(dist - 0.91986) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice coefficient / SÃ¸rensen-Dice Index\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\text{Dice}(A, B) = \\frac{2 |A \\cap B|}{|A| + |B|} \n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &A \\text{ and } B \\text{ represent the two sets being compared} \\\\\n",
    "    &|A| \\text{ and } |B| \\text{ represent the cardinality (number of elements) of the sets} \\\\\n",
    "    &\\text{and } |A \\cap B| \\text{ represents the size of the intersection of the two sets}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8888888888888888"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dice(text_a: str, text_b: str) -> float:\n",
    "    vec_1, vec_2 = text_to_vec([text_a, text_b])\n",
    "    count_union = len(([(a, b) for a, b in zip(vec_1, vec_2) if a > 0 and b > 0]))\n",
    "    count_1 = len([a for a in vec_1 if a > 0])\n",
    "    count_2 = len(([b for b in vec_2 if b > 0]))\n",
    "    return 2*count_union / (count_1 + count_2)\n",
    "\n",
    "dice(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = dice(text_a, text_b)\n",
    "assert(abs(dist - 0.8888888) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &d(x,y) \\text{ is the Euclidean distance} \\\\\n",
    "    &x_i, y_i \\text{ are the values of the i-th dimension of vectors } x \\text{ and } y \\\\\n",
    "    &n \\text{ is the number of dimensions in the vectors}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(text_a: str, text_b: str) -> float:\n",
    "    vec_1, vec_2 = text_to_vec([text_a, text_b])\n",
    "    vec_1 = np.array(vec_1)\n",
    "    vec_2 = np.array(vec_2)\n",
    "    return np.linalg.norm(vec_1 - vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "dist = euclidean_distance(text_a, text_b)\n",
    "print(dist)\n",
    "assert(abs(dist - 1.4142135) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCS - Longest Common Subsequence\n",
    "\n",
    "Longest, common, continuous subsequence of two sequences, aka \"the longest substring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "\n",
    "def lcs(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    m = len(seq_a)\n",
    "    n = len(seq_b)\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq_a[i-1] == seq_b[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def word_lcs(text_a: str, text_b: str) -> int:\n",
    "    seq_a, seq_b = preprocess(text_a).split(), preprocess(text_b).split()\n",
    "    return lcs(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert lcs(\"banana\", \"ananas\") == 5\n",
    "assert word_lcs(text_a, text_b) == 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance\n",
    "\n",
    "The minimal number of operations that needs to be performed in order to turn sequence A into sequence B.\n",
    "\n",
    "Available operations:\n",
    "\n",
    "* Replace element\n",
    "* Remove element\n",
    "* Add element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def levenshtein(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    m = len(seq_a)\n",
    "    n = len(seq_b)\n",
    "    dp = [[float(\"inf\") for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    parent = [[-1 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for i in range(n + 1):\n",
    "        dp[0][i] = i\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq_a[i-1] == seq_b[j - 1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "                parent[i][j] = 0\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j-1] + 1, dp[i-1][j] + 1, dp[i][j - 1] + 1)\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "def word_levenshtein(text_a: str, text_b: str) -> int:\n",
    "    seq_a, seq_b = preprocess(text_a).split(), preprocess(text_b).split()\n",
    "    return levenshtein(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert levenshtein(\"banana\", \"ananas\") == 2\n",
    "assert word_levenshtein(text_a, text_b) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
