{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rozwiązanie laboratorium 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funkcje konwertujące"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "def string_to_int(string):\n",
    "    val = 0\n",
    "    for x in string:\n",
    "        val = val << 1 | (0 if x == '0' else 1)\n",
    "    return val\n",
    "\n",
    "\n",
    "def int_to_string(x, no_of_bits):\n",
    "    string = []\n",
    "    mask = 1 << (no_of_bits - 1)\n",
    "    for _ in range(no_of_bits):\n",
    "        string.append(\"1\" if x & mask else \"0\")\n",
    "        mask >>= 1\n",
    "    return \"\".join(string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Wczytytywanie i odczytywanie z plików binarnych/tekstowych\n",
    "\n",
    "Na końcu każdego pliku dodaje dodatkowe bity, aby długość pliku była podzielna przez 8. Ostatni bajt zawiera informacje ile dokładnie bitów zostało dodane.\n",
    "Przy wczytywaniu z pliku te ostatnie bity są pomijane\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "def read_file_to_string(filename):\n",
    "    with open(filename, \"r\", encoding=\"UTF-8\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_binary_file_to_string(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        bit_data = f.read()\n",
    "\n",
    "    data = []\n",
    "    for bit in bit_data[:-2]:\n",
    "        data.append(int_to_string(bit, 8))\n",
    "    last = \"\"\n",
    "    mask = 1 << 7\n",
    "    if bit_data[-1]:\n",
    "        for j in range(bit_data[-1]):\n",
    "            last += \"1\" if mask & bit_data[-2] else \"0\"\n",
    "            mask >>= 1\n",
    "    else:\n",
    "        for j in range(8):\n",
    "            last += \"1\" if mask & bit_data[-2] else \"0\"\n",
    "            mask >>= 1\n",
    "    data.append(last)\n",
    "    return \"\".join(data)\n",
    "\n",
    "\n",
    "def write_string_to_binary_file(filename, text):\n",
    "    b = bytearray()\n",
    "    for i in range(0, len(text), 8):\n",
    "        b.append(string_to_int(text[i:i+8]))\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "def add_last_bits(text):\n",
    "    padding_length = len(text) % 8\n",
    "    text += \"0\" * ((8 - padding_length)%8)\n",
    "    text += int_to_string(padding_length, 8)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Statyczne drzewo Huffmana\n",
    "\n",
    "Przy kompresowaniu pliku początkowo zakodowane litery zapisuje jako zera i jedynki (znaki ascii). Też do takiej formy\n",
    "zapisuje wczytywany plik. Dopiero potem znaki odpowiednio konweruje na liczby i zapisuje do plików. Używanie stringów było jednak najwygodniejsze."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "class StaticNode:\n",
    "    def __init__(self, character=None):\n",
    "        self.character = character\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class StaticHuffman:\n",
    "    def __init__(self):\n",
    "        self.tree_root = None\n",
    "        self.frequency_dict = {}\n",
    "        self.code_dict = {}\n",
    "\n",
    "    def build_frequency_dict(self, data):\n",
    "        for c in data:\n",
    "            if c not in self.frequency_dict:\n",
    "                self.frequency_dict[c] = 1\n",
    "            else:\n",
    "                self.frequency_dict[c] += 1\n",
    "\n",
    "    def build_tree(self):\n",
    "        pq = PriorityQueue()\n",
    "        for c in self.frequency_dict:\n",
    "            pq.put((self.frequency_dict[c], c, StaticNode(c)))\n",
    "\n",
    "        while True:\n",
    "            freq1, str1, node1 = pq.get()\n",
    "            if pq.empty():\n",
    "                self.tree_root = node1\n",
    "                return\n",
    "            freq2, str2, node2 = pq.get()\n",
    "            new_node = StaticNode()\n",
    "            new_node.left = node1\n",
    "            new_node.right = node2\n",
    "            pq.put((freq1 + freq2, str1 + str2, new_node))\n",
    "\n",
    "    def code_characters(self):\n",
    "        def traverse_tree(node, code=\"\"):\n",
    "            if node.character is not None:\n",
    "                self.code_dict[node.character] = code\n",
    "            else:\n",
    "                traverse_tree(node.left, code + '0')\n",
    "                traverse_tree(node.right, code + '1')\n",
    "\n",
    "        traverse_tree(self.tree_root, code=\"\")\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        self.build_frequency_dict(text)\n",
    "        self.build_tree()\n",
    "        self.code_characters()\n",
    "        encoded = []\n",
    "        for c in text:\n",
    "            encoded.append(self.code_dict[c])\n",
    "        return add_last_bits(\"\".join(encoded))\n",
    "\n",
    "    def decode_text(self, text):\n",
    "        ind = 0\n",
    "        decoded = []\n",
    "        while ind < len(text):\n",
    "            ptr = self.tree_root\n",
    "            while ptr.character is None:\n",
    "                ptr = ptr.left if text[ind] == '0' else ptr.right\n",
    "                ind += 1\n",
    "            decoded.append(ptr.character)\n",
    "        return \"\".join(decoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dynamiczne drzewo Huffmana\n",
    "\n",
    "Podobnie jak w statycznym drzewie Huffmana w samym programie używam stringów.\n",
    "Moja implementacja bazuje na algorytmie FGK(Faller-Gallager-Knuth). Otrzymana struktura nie jest najszybsza.\n",
    "Można by to poprawić, gdyby struktury self.node_weights i self.leaf_weights ( atrybuty klasy AdaptiveHuffman ), byłyby listami posortowanych\n",
    "zbiorór (np. drzew AVL), a nie zwykłych zbiorów.\n",
    "Ponadto, jako że przy dekodowaniu w dynamicznym drzewie Huffmana drzewo jest tworzone na nowo i nie wiadomo jakie litery mogą się pojawić, w momencie, gdy\n",
    "po raz pierwszy napotykamy musimy ją zapisać w kodzie \"normalnie\". W pythonie jest dostępna funkcja ord(), która dla znaku zwraca liczbę z kodu ascii\n",
    "Działa ona jednak dla każdej znaku z UTF-8 ( większość plików chociażby z gutenberg.ord jest w takim formacie). Niektóre jednak znaki nie mieszczą się na jednym bajcie, lecz dopiero na dwóch. Dlatego przy kodowaniu nowo napotkanego znaku używałem zawsze 16 bitów. Dany znak jest tak zapisywany tylko raz, więc nie ma to znaczenia dla plików, które są większe niż kilka kilobajtów, lecz dla małych plików, w których jest dużo różnych znaków współczynnik kompresji może być bardzo niski."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "class AdaptiveNode:\n",
    "    def __init__(self, index, weight, character, external):\n",
    "        self.index = index\n",
    "        self.weight = weight\n",
    "        self.character = character\n",
    "        self.external = external\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "\n",
    "\n",
    "def interchange(node, change):\n",
    "    if change != node:\n",
    "        change.index, node.index = node.index, change.index\n",
    "        parent_change, parent_node = change.parent, node.parent\n",
    "        if parent_change.left == change:\n",
    "            if parent_node.left == node:\n",
    "                parent_change.left, parent_node.left = node, change\n",
    "            else:\n",
    "                parent_change.left, parent_node.right = node, change\n",
    "        else:\n",
    "            if parent_node.left == node:\n",
    "                parent_change.right, parent_node.left = node, change\n",
    "            else:\n",
    "                parent_change.right, parent_node.right = node, change\n",
    "        node.parent, change.parent = parent_change, parent_node\n",
    "\n",
    "\n",
    "def update_weight(node, node_dict):\n",
    "    node_dict[node.weight].remove(node)\n",
    "    if node.weight + 1 >= len(node_dict):\n",
    "        node_dict.append(set())\n",
    "    node_dict[node.weight + 1].add(node)\n",
    "\n",
    "\n",
    "class AdaptiveHuffman:\n",
    "    def __init__(self):\n",
    "        self.root = AdaptiveNode(1000, 0, \"NYT\", True)\n",
    "        self.NYT = self.root\n",
    "        self.free_index = 999\n",
    "        self.leaves = {}\n",
    "        self.node_weights = [set(), set()]\n",
    "        self.leaf_weights = [set(), set()]\n",
    "\n",
    "    def get_leaf_code(self, node):\n",
    "        code = []\n",
    "        while node != self.root:\n",
    "            code.append(\"0\" if node == node.parent.left else \"1\")\n",
    "            node = node.parent\n",
    "        return \"\".join(code)[::-1]\n",
    "\n",
    "    def add_new_node(self, char):\n",
    "        right_child = AdaptiveNode(self.free_index, 1, char, True)\n",
    "        self.free_index -= 1\n",
    "        left_child = AdaptiveNode(self.free_index, 0, \"NYT\", True)\n",
    "        self.free_index -= 1\n",
    "\n",
    "        internal, self.NYT = self.NYT, left_child\n",
    "\n",
    "        internal.weight = 1\n",
    "        internal.character = \"\"\n",
    "        internal.external = False\n",
    "        internal.left = left_child\n",
    "        internal.right = right_child\n",
    "\n",
    "        right_child.parent = internal\n",
    "        left_child.parent = internal\n",
    "\n",
    "        self.leaves[char] = right_child\n",
    "        self.node_weights[1].add(right_child)\n",
    "        if internal != self.root:\n",
    "            self.node_weights[1].add(internal)\n",
    "\n",
    "        self.leaf_weights[1].add(right_child)\n",
    "        self.update(internal)\n",
    "\n",
    "    def update(self, node):\n",
    "        while node != self.root:\n",
    "            if node.parent.left == self.NYT:\n",
    "                change = max(self.leaf_weights[node.weight], key=lambda item: item.index)\n",
    "                interchange(node, change)\n",
    "            else:\n",
    "                change = max(self.node_weights[node.weight], key=lambda item: item.index)\n",
    "                interchange(node, change)\n",
    "\n",
    "            if node.external:\n",
    "                update_weight(node, self.leaf_weights)\n",
    "            update_weight(node, self.node_weights)\n",
    "            node.weight += 1\n",
    "            node = node.parent\n",
    "\n",
    "        self.root.weight += 1\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        encoded = []\n",
    "        for c in text:\n",
    "            if c not in self.leaves:\n",
    "                encoded.append(self.get_leaf_code(self.NYT))\n",
    "                encoded.append(int_to_string(ord(c), 16))\n",
    "                self.add_new_node(c)\n",
    "            else:\n",
    "                encoded.append(self.get_leaf_code(self.leaves[c]))\n",
    "                self.update(self.leaves[c])\n",
    "\n",
    "        return add_last_bits(\"\".join(encoded))\n",
    "\n",
    "\n",
    "    def decode_text(self, text):\n",
    "        decoded = []\n",
    "        ind = 0\n",
    "        while ind < len(text):\n",
    "            ptr = self.root\n",
    "            while not ptr.external:\n",
    "                ptr = ptr.left if text[ind] == '0' else ptr.right\n",
    "                ind += 1\n",
    "\n",
    "            if ptr == self.NYT:\n",
    "                new_char = chr(string_to_int(text[ind:ind+16]))\n",
    "                decoded.append(new_char)\n",
    "                self.add_new_node(new_char)\n",
    "                ind += 16\n",
    "            else:\n",
    "                decoded.append(ptr.character)\n",
    "                self.update(ptr)\n",
    "        return \"\".join(decoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Współczynniki kompresji i testy czasowe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os.path import getsize\n",
    "\n",
    "files1 = [\"Gutenberg_files/\" + f for f in listdir(\"Gutenberg_files\") if isfile(join(\"Gutenberg_files\", f))]\n",
    "files2 = [\"random_files/\" + f for f in listdir(\"random_files\") if isfile(join(\"random_files\", f))]\n",
    "files3 = [\"Linux_kernel/\" + f for f in listdir(\"Linux_kernel\") if isfile(join(\"Linux_kernel\", f))]\n",
    "files = files1 + files2 + files3\n",
    "\n",
    "static_tree_compression_time = []\n",
    "dynamic_tree_compression_time = []\n",
    "static_tree_decompression_time = []\n",
    "dynamic_tree_decompression_time = []\n",
    "static_tree_compression_ratio = []\n",
    "dynamic_tree_compression_ratio = []\n",
    "\n",
    "static_tree_correctness = []\n",
    "dynamic_tree_correctness = []\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    static_tree = StaticHuffman()\n",
    "    adaptive_encoder = AdaptiveHuffman()\n",
    "    adaptive_decoder = AdaptiveHuffman()\n",
    "    f_content = read_file_to_string(file)\n",
    "    t = perf_counter()\n",
    "    compressed = static_tree.encode_text(f_content)\n",
    "    t = perf_counter() - t\n",
    "    static_tree_compression_time.append(t)\n",
    "\n",
    "    write_string_to_binary_file(\"compressed.bin\", compressed)\n",
    "    to_decode = read_binary_file_to_string(\"compressed.bin\")\n",
    "    t = perf_counter()\n",
    "    decompressed = static_tree.decode_text(to_decode)\n",
    "    t = perf_counter() - t\n",
    "    static_tree_decompression_time.append(t)\n",
    "    static_tree_correctness.append(decompressed == f_content)\n",
    "    static_tree_compression_ratio.append(1 - (getsize('compressed.bin') / getsize(file)))\n",
    "\n",
    "\n",
    "    t = perf_counter()\n",
    "    compressed = adaptive_encoder.encode_text(f_content)\n",
    "    t = perf_counter() - t\n",
    "    dynamic_tree_compression_time.append(t)\n",
    "\n",
    "    write_string_to_binary_file(\"compressed.bin\", compressed)\n",
    "    to_decode = read_binary_file_to_string(\"compressed.bin\")\n",
    "\n",
    "    t = perf_counter()\n",
    "    decompressed = adaptive_decoder.decode_text(to_decode)\n",
    "    t = perf_counter() - t\n",
    "\n",
    "    dynamic_tree_decompression_time.append(t)\n",
    "    dynamic_tree_correctness.append(decompressed == f_content)\n",
    "    dynamic_tree_compression_ratio.append(1 - (getsize('compressed.bin') / getsize(file)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statyczne kodowanie Huffmana"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "                            File Name  Compression time  Decompression time  \\\n0            Gutenberg_files/book.txt              0.00                0.00   \n1      Gutenberg_files/mickiewicz.txt              0.02                0.02   \n2       Gutenberg_files/moby_dick.txt              0.68                0.98   \n3      Gutenberg_files/tom_sawyer.txt              0.08                0.12   \n4   random_files/file_size_1000kb.txt              0.52                1.02   \n5    random_files/file_size_100kb.txt              0.03                0.09   \n6     random_files/file_size_10kb.txt              0.00                0.01   \n7      random_files/file_size_1kb.txt              0.00                0.00   \n8           Linux_kernel/makefile.txt              0.00                0.00   \n9             Linux_kernel/merged.txt              0.32                0.97   \n10           Linux_kernel/smpboot.txt              0.00                0.01   \n11         Linux_kernel/workqueue.txt              0.04                0.11   \n\n    Compression ratio  Check correctness  \n0                0.45               True  \n1                0.43               True  \n2                0.45               True  \n3                0.45               True  \n4                0.14               True  \n5                0.14               True  \n6                0.14               True  \n7                0.16               True  \n8                0.33               True  \n9                0.36               True  \n10               0.38               True  \n11               0.37               True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Compression time</th>\n      <th>Decompression time</th>\n      <th>Compression ratio</th>\n      <th>Check correctness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gutenberg_files/book.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gutenberg_files/mickiewicz.txt</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>0.43</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gutenberg_files/moby_dick.txt</td>\n      <td>0.68</td>\n      <td>0.98</td>\n      <td>0.45</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Gutenberg_files/tom_sawyer.txt</td>\n      <td>0.08</td>\n      <td>0.12</td>\n      <td>0.45</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>random_files/file_size_1000kb.txt</td>\n      <td>0.52</td>\n      <td>1.02</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>random_files/file_size_100kb.txt</td>\n      <td>0.03</td>\n      <td>0.09</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>random_files/file_size_10kb.txt</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>random_files/file_size_1kb.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.16</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Linux_kernel/makefile.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.33</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Linux_kernel/merged.txt</td>\n      <td>0.32</td>\n      <td>0.97</td>\n      <td>0.36</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Linux_kernel/smpboot.txt</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.38</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Linux_kernel/workqueue.txt</td>\n      <td>0.04</td>\n      <td>0.11</td>\n      <td>0.37</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'File Name': files,\n",
    "    'Compression time': np.round(np.array(static_tree_compression_time), 2),\n",
    "    'Decompression time': np.round(np.array(static_tree_decompression_time), 2),\n",
    "    'Compression ratio': np.round(np.array(static_tree_compression_ratio), 2),\n",
    "    'Check correctness': static_tree_correctness\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.style.set_caption(\"Static Huffman Tree results\")\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dynamiczne kodowanie Huffmana"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "data": {
      "text/plain": "                            File Name  Compression time  Decompression time  \\\n0            Gutenberg_files/book.txt              0.01                0.01   \n1      Gutenberg_files/mickiewicz.txt              0.14                0.18   \n2       Gutenberg_files/moby_dick.txt             13.43               11.64   \n3      Gutenberg_files/tom_sawyer.txt              1.55                2.98   \n4   random_files/file_size_1000kb.txt             13.48               12.42   \n5    random_files/file_size_100kb.txt              1.26                1.17   \n6     random_files/file_size_10kb.txt              0.14                0.13   \n7      random_files/file_size_1kb.txt              0.05                0.05   \n8           Linux_kernel/makefile.txt              0.01                0.01   \n9             Linux_kernel/merged.txt             11.42               10.37   \n10           Linux_kernel/smpboot.txt              0.14                0.11   \n11         Linux_kernel/workqueue.txt              1.28                1.24   \n\n    Compression ratio  Check correctness  \n0                0.30               True  \n1                0.42               True  \n2                0.45               True  \n3                0.45               True  \n4                0.14               True  \n5                0.14               True  \n6                0.12               True  \n7                0.07               True  \n8                0.14               True  \n9                0.36               True  \n10               0.36               True  \n11               0.37               True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Compression time</th>\n      <th>Decompression time</th>\n      <th>Compression ratio</th>\n      <th>Check correctness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gutenberg_files/book.txt</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.30</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gutenberg_files/mickiewicz.txt</td>\n      <td>0.14</td>\n      <td>0.18</td>\n      <td>0.42</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gutenberg_files/moby_dick.txt</td>\n      <td>13.43</td>\n      <td>11.64</td>\n      <td>0.45</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Gutenberg_files/tom_sawyer.txt</td>\n      <td>1.55</td>\n      <td>2.98</td>\n      <td>0.45</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>random_files/file_size_1000kb.txt</td>\n      <td>13.48</td>\n      <td>12.42</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>random_files/file_size_100kb.txt</td>\n      <td>1.26</td>\n      <td>1.17</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>random_files/file_size_10kb.txt</td>\n      <td>0.14</td>\n      <td>0.13</td>\n      <td>0.12</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>random_files/file_size_1kb.txt</td>\n      <td>0.05</td>\n      <td>0.05</td>\n      <td>0.07</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Linux_kernel/makefile.txt</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.14</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Linux_kernel/merged.txt</td>\n      <td>11.42</td>\n      <td>10.37</td>\n      <td>0.36</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Linux_kernel/smpboot.txt</td>\n      <td>0.14</td>\n      <td>0.11</td>\n      <td>0.36</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Linux_kernel/workqueue.txt</td>\n      <td>1.28</td>\n      <td>1.24</td>\n      <td>0.37</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'File Name': files,\n",
    "    'Compression time': np.round(np.array(dynamic_tree_compression_time), 2),\n",
    "    'Decompression time': np.round(np.array(dynamic_tree_decompression_time), 2),\n",
    "    'Compression ratio': np.round(np.array(dynamic_tree_compression_ratio), 2),\n",
    "    'Check correctness': static_tree_correctness\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorytm o zmiennym bloku kompresji LZW (Lempel–Ziv–Welch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorytm zakłada, że przy kompresji i dekompresji znany jest alfabet pojedynczych znaków jakie można napotkać w pliku. Jednak dla liczb powyżej 130 rzucała błędy, więc\n",
    "słownik stworzyłem iterując po wszystkich plikach jakie będę kompresował."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "def gather_characters(all_files):\n",
    "    d = set()\n",
    "    for f in all_files:\n",
    "        text = read_file_to_string(f)\n",
    "        for x in text:\n",
    "            d.add(x)\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "characters = list(gather_characters(files))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorytm ten w przeciwieństwie do poprzednich nie zwraca od razu ciągu zer i jedynek, tylko listę, w której są pojedyncze znaki albo kody słów, które powstały przez konkatenacje\n",
    "tych znaków. W algorytmie dodałem warunek, że ilość kodów nie może przekraczać liczby 65535, żeby wszystkie kody mogły zmieścić się na dwóch bajtach. To założenie nie wpływa znacząco na wynik, a późniejsze wczytywanie do pliku binarnego i dekompresja jest znacznie łatwiejsza."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "def read_binary_file_to_lzw_decoding(filename):\n",
    "    text = read_binary_file_to_string(filename)\n",
    "    array = []\n",
    "    ind = 0\n",
    "    while ind < len(text):\n",
    "        if text[ind] == \"1\":\n",
    "            ind += 1\n",
    "            array.append(chr(string_to_int(text[ind:ind+16])))\n",
    "        else:\n",
    "            ind += 1\n",
    "            array.append(string_to_int(text[ind:ind+16]))\n",
    "        ind += 16\n",
    "\n",
    "    return array\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def write_lzw_code_to_binary(filename, coded):\n",
    "    binary = []\n",
    "    for x in coded:\n",
    "        if isinstance(x, str):\n",
    "            binary.append(\"1\")\n",
    "            binary.append(int_to_string(ord(x), 16))\n",
    "        else:\n",
    "            binary.append(\"0\")\n",
    "            binary.append(int_to_string(x, 16))\n",
    "    write_string_to_binary_file(\"compressed.bin\", add_last_bits(\"\".join(binary)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def encoding(text):\n",
    "    dictionary = {x:x for x in characters}\n",
    "    last = 1\n",
    "    encoded = []\n",
    "    word = \"\"\n",
    "    for x in text:\n",
    "        if word + x in dictionary:\n",
    "            word += x\n",
    "        else:\n",
    "            encoded.append(dictionary[word])\n",
    "            if last < 65535:\n",
    "                dictionary[word + x] = last\n",
    "                last += 1\n",
    "            word = x\n",
    "    if word:\n",
    "        encoded.append(dictionary[word])\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decoding(text):\n",
    "    decoded = []\n",
    "    dictionary = {x:x for x in characters}\n",
    "    last = 1\n",
    "\n",
    "    word = text[0]\n",
    "    decoded.append(word)\n",
    "    for x in text[1:]:\n",
    "        if x in dictionary:\n",
    "            entry = dictionary[x]\n",
    "        else:\n",
    "            entry = word + word[0]\n",
    "        decoded.append(entry)\n",
    "        if last < 65535:\n",
    "            dictionary[last] = word + entry[0]\n",
    "            last += 1\n",
    "        word = entry\n",
    "    return \"\".join(decoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lzw_compression_time = []\n",
    "lzw_decompression_time = []\n",
    "lzw_compression_ratio = []\n",
    "lzw_correctness = []\n",
    "\n",
    "for file in files:\n",
    "    f_content = read_file_to_string(file)\n",
    "    t = perf_counter()\n",
    "    compressed = encoding(f_content)\n",
    "    t = perf_counter() - t\n",
    "    lzw_compression_time.append(t)\n",
    "\n",
    "    write_lzw_code_to_binary(\"compressed.bin\", compressed)\n",
    "    to_decode = read_binary_file_to_lzw_decoding(\"compressed.bin\")\n",
    "    t = perf_counter()\n",
    "    decompressed = decoding(to_decode)\n",
    "    t = perf_counter() - t\n",
    "    lzw_decompression_time.append(t)\n",
    "    lzw_correctness.append(decompressed == f_content)\n",
    "    lzw_compression_ratio.append(1 - (getsize('compressed.bin') / getsize(file)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "                            File Name  Compression time  Decompression time  \\\n0            Gutenberg_files/book.txt              0.00                0.00   \n1      Gutenberg_files/mickiewicz.txt              0.01                0.01   \n2       Gutenberg_files/moby_dick.txt              1.89                0.15   \n3      Gutenberg_files/tom_sawyer.txt              0.07                0.02   \n4   random_files/file_size_1000kb.txt              0.58                0.27   \n5    random_files/file_size_100kb.txt              0.08                0.03   \n6     random_files/file_size_10kb.txt              0.00                0.00   \n7      random_files/file_size_1kb.txt              0.00                0.00   \n8           Linux_kernel/makefile.txt              0.00                0.00   \n9             Linux_kernel/merged.txt              0.51                0.08   \n10           Linux_kernel/smpboot.txt              0.01                0.00   \n11         Linux_kernel/workqueue.txt              0.05                0.01   \n\n    Compression ratio  Check correctness  \n0               -0.31               True  \n1                0.19               True  \n2                0.56               True  \n3                0.51               True  \n4               -0.06               True  \n5               -0.19               True  \n6               -0.71               True  \n7               -0.38               True  \n8               -0.31               True  \n9                0.71               True  \n10               0.30               True  \n11               0.53               True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Compression time</th>\n      <th>Decompression time</th>\n      <th>Compression ratio</th>\n      <th>Check correctness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gutenberg_files/book.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>-0.31</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gutenberg_files/mickiewicz.txt</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.19</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gutenberg_files/moby_dick.txt</td>\n      <td>1.89</td>\n      <td>0.15</td>\n      <td>0.56</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Gutenberg_files/tom_sawyer.txt</td>\n      <td>0.07</td>\n      <td>0.02</td>\n      <td>0.51</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>random_files/file_size_1000kb.txt</td>\n      <td>0.58</td>\n      <td>0.27</td>\n      <td>-0.06</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>random_files/file_size_100kb.txt</td>\n      <td>0.08</td>\n      <td>0.03</td>\n      <td>-0.19</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>random_files/file_size_10kb.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>-0.71</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>random_files/file_size_1kb.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>-0.38</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Linux_kernel/makefile.txt</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>-0.31</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Linux_kernel/merged.txt</td>\n      <td>0.51</td>\n      <td>0.08</td>\n      <td>0.71</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Linux_kernel/smpboot.txt</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.30</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Linux_kernel/workqueue.txt</td>\n      <td>0.05</td>\n      <td>0.01</td>\n      <td>0.53</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'File Name': files,\n",
    "    'Compression time': np.round(np.array(lzw_compression_time), 2),\n",
    "    'Decompression time': np.round(np.array(lzw_decompression_time), 2),\n",
    "    'Compression ratio': np.round(np.array(lzw_compression_ratio), 2),\n",
    "    'Check correctness': lzw_correctness\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorytm lzw zyskuje, gdy w pliku pojawia się dużo powtarzających się wzorów. Możliwe jest, że uzyska ujemny współczynnik kompresji, gdyż zapisuje otrzymane kody na dwóch bajtach.\n",
    "Widać, że jest nieskuteczny dla małych plików i plików losowych, za to bardzo wysoki współćzynnik kompresji uzyskały pliku z jądra linuxa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Porównanie z huffmanem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg_files/moby_dick.txt\n",
      "Gutenberg_files/tom_sawyer.txt\n",
      "Linux_kernel/merged.txt\n",
      "Linux_kernel/workqueue.txt\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x, y, z, file_name in zip(dynamic_tree_compression_ratio, static_tree_compression_ratio, lzw_compression_ratio, files):\n",
    "    if z == max(x, y, z):\n",
    "        count += 1\n",
    "        print(file_name)\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Najlepszy okazał się łącznie dla czterech plików(duże nielosowe pliki), a był gorszy dla plików poniżej 10kb i wszystkich losowych"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
