{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rozwiązanie Lab 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open(\"lines.txt\", \"r\", encoding=\"UTF-8\") as f1:\n",
    "    unprocessed_e_mails = f1.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Klasa EMAIL\n",
    "Przechowuje informacje o jednym mailu z pliku lines\n",
    "self.email - treść maila\n",
    "self.word_dict - reprezentacja maila bag_of_words, można to utożsamiać z wektorem, lecz jest to dict\n",
    "self.index_vector - przechowuje id słów, które występują w tym tekście, tej reprezentacji używam przy metrykach dice, lcs, levenshtein\n",
    "self.vector_norm - norma euklidesowa wektora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class EMAIL:\n",
    "    def __init__(self, email: str, word_dict: dict[str, int]):\n",
    "        self.email: str = email\n",
    "        self.word_dict : dict[str, int] = word_dict\n",
    "        self.index_vector = None\n",
    "        self.vector_norm = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing tekstu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "\n",
    "def process_text(email: str) -> EMAIL:\n",
    "    text = \"\".join(list(map(lambda c: \" \" if c in punctuation else c, email)))\n",
    "    words = [word.lower() for word in word_tokenize(text)]\n",
    "    return EMAIL(email, dict(FreqDist(words)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "emails = list(map(process_text, unprocessed_e_mails))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tworzenie alfabetu (biorę wszystkie słowa jakie występują w tekstach)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17804\n",
      "6751\n"
     ]
    }
   ],
   "source": [
    "alphabet = {}\n",
    "count = {}\n",
    "i = 0\n",
    "for e in emails:\n",
    "    for word in e.word_dict:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = i\n",
    "            count[word] = 1\n",
    "            i += 1\n",
    "        else:\n",
    "            count[word] += 1\n",
    "M1 = len(alphabet)\n",
    "N1 = len(emails)\n",
    "print(M1)\n",
    "print(N1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for e in emails:\n",
    "    indices = list(map(lambda x: alphabet[x], e.word_dict.keys()))\n",
    "    e.index_vector = np.sort(np.array(indices))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tworzenie macierzy, w której każda kolumna reprezentuje jakieś słowo jako wektor. Oczywiście jest to macierz rzadka.\n",
    "Argument words jest to alfabet słów, które występują w mailach w liście mails. M to rozmiar alfabetu a N to ilość maili.\n",
    "Argument normalize umożliwia stworzenie macierzy w której wektory są znormalizowane (dzięki temu można szybko obliczyć wartości metryki cosinusowej)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_sparse_matrix(mails: list[EMAIL], words: dict[str, int], M: int, N: int, normalize=False, idf_values=None):\n",
    "    nonzero_entries = sum(len(e.index_vector) for e in mails)\n",
    "    data = np.zeros(nonzero_entries)\n",
    "    row = np.copy(data)\n",
    "    col = np.copy(data)\n",
    "    ind = 0\n",
    "    for i, e in enumerate(mails):\n",
    "        start = ind\n",
    "        for w in e.word_dict:\n",
    "            data[ind] = e.word_dict[w] * (1 if idf_values is None else idf_values[w])\n",
    "            row[ind] = words[w]\n",
    "            col[ind] = i\n",
    "            ind += 1\n",
    "        length = np.linalg.norm(data[start:ind])\n",
    "        e.vector_norm = length\n",
    "        if normalize:\n",
    "            data[start:ind] /= length\n",
    "\n",
    "    return sparse.csr_matrix((data, (row, col)), shape=(M, N), dtype=float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "bag_of_words = create_sparse_matrix(emails, alphabet, M1, N1)\n",
    "bag_of_words_n = create_sparse_matrix(emails, alphabet, M1, N1, normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementacja metryk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metryka cosinusowa\n",
    "Od razu obliczam macierz która zawiera wartość metryki dla każdej pary tekstów. Wyjściowo macierz ma wymiary N x N, gdzie N to liczba tekstów. Obliczanie wartości metryk za każdym razem dla jakiś dwóch teksów zajmowało, bardzo długo, lecz wykorzystując mnożenie macierzy rzadkich, można wartości tych metryk obliczyć bardzo szybko.\n",
    "Podana macierz musi być już znormalizowana."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def cosine(matrix):\n",
    "    return np.array((matrix.T @ matrix).todense())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metryka euklidesowa\n",
    "Postępuje analogicznie jak w przypadku metryki cosinusowej i obliczam wartości metryki korzystając z działań na macierzach."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def euclidian(matrix, e_mails, N):\n",
    "    X = np.array([e.vector_norm**2 for e in e_mails]).reshape((N, 1)) @ np.ones(N).reshape((1, N))\n",
    "    return np.sqrt(np.abs(X + X.T - 2*np.array((matrix.T @ matrix).todense())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dla pozostałych metryk nie da się specjalnie przyśpieszyć obliczania ich wartości. Klasteryzacje z użyciem tych metryk będę przeprowadzał na dużo mniejszym zbiorze tekstów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Najdłuższy wspólny podciąg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def lcs(text1: EMAIL, text2: EMAIL) -> float:\n",
    "    seq_a = text1.index_vector\n",
    "    seq_b = text2.index_vector\n",
    "    m = len(seq_a)\n",
    "    n = len(seq_b)\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq_a[i-1] == seq_b[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return 1 - (dp[-1][-1] / max(m, n))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Odległość edycyjna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def levenshtein(text1: EMAIL, text2: EMAIL) -> float:\n",
    "    seq_a = text1.index_vector\n",
    "    seq_b = text2.index_vector\n",
    "    m = len(seq_a)\n",
    "    n = len(seq_b)\n",
    "    dp = [[float(\"inf\") for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for i in range(n + 1):\n",
    "        dp[0][i] = i\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq_a[i-1] == seq_b[j - 1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j-1] + 1, dp[i-1][j] + 1, dp[i][j - 1] + 1)\n",
    "\n",
    "    return dp[-1][-1] / max(m, n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Współczynnik DICE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def dice(text1: EMAIL, text2: EMAIL) -> float:\n",
    "    seq_a = text1.index_vector\n",
    "    seq_b = text2.index_vector\n",
    "    count_union = len([x for x in seq_a if x in seq_b])\n",
    "    count_1 = len(seq_a)\n",
    "    count_2 = len(seq_b)\n",
    "    return 1 - (2*count_union / (count_1 + count_2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Klasteryzacja\n",
    "\n",
    "Do klasteryzacji używam algorytmu k-means. Jest on jednak trochę zmodyfikowany. W oryginalnym algorytmie na początku losowo wybierało się k przedstawicieli grup. Każdy tekst trafiał do grupy do której przedstawiciela miał najbliżej według danej metryki. Następnie kolejnych przedstawicieli się już nie losuje, tylko bierze się średnią z wartości w danym klasterze. Jednak implementacja takiego trudna byłaby trudna, gdyż wtedy pojawiałyby się nowe punkty, a reprezentacja tekstów jako wielowymiarów wektórów, byłaby wymagająca obliczeniowo i prawdopodobnie algorytm nie policzyłby się dla całego zbioru maili. Poza tym pojęcie średniej zmienia zależnie od tego jakiej użyje się metryki. Ponadto przy metryce lcs, dice i levenshtein'a niemożliwe byłoby porównanie tekstów do wektora który powstał ze średniej. Dlatego jako średnią z klustera wybieram po prostu tekst, który znajduje się tym zbiorze i minimalizuje wariancje w obrębie tego klustera. Do oceny klasteryzacji używam indeksu Davies-Bouldine'a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6751, 6751)\n",
      "(6751, 6751)\n",
      "CPU times: total: 3.97 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cosine_metric = cosine(bag_of_words_n)\n",
    "euclidian_metric = euclidian(bag_of_words, emails, N1)\n",
    "print(cosine_metric.shape)\n",
    "print(euclidian_metric.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dice_metric = np.array([[dice(mail1, mail2) for mail1 in emails[:200]] for mail2 in emails[:200]])\n",
    "lcs_metric = np.array([[lcs(mail1, mail2) for mail1 in emails[:200]] for mail2 in emails[:200]])\n",
    "edit_metric = np.array([[levenshtein(mail1, mail2) for mail1 in emails[:200]] for mail2 in emails[:200]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def find_centroid(cluster, metric):\n",
    "    dists = metric[np.ix_(cluster, cluster)]\n",
    "    variances = np.power(np.sum(dists, axis=1), 2)\n",
    "    return cluster[np.argmin(variances)], np.min(variances)\n",
    "\n",
    "\n",
    "def k_means_cluster(mails, metric=euclidian_metric, max_iter=20, k=10):\n",
    "    n = metric.shape[0]\n",
    "    a = np.arange(n)\n",
    "    np.random.shuffle(a)\n",
    "    cluster_means = a[:k]\n",
    "    best_clustering = cluster_means\n",
    "    best_variance = float(\"inf\")\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        clusters = [[cluster_means[i]] for i in range(k)]\n",
    "        for i, mail in enumerate(mails):\n",
    "            if i not in cluster_means:\n",
    "                clusters[np.argmin(metric[i, cluster_means])].append(i)\n",
    "\n",
    "        cluster_variance = 0\n",
    "        prev_clustering = cluster_means.copy()\n",
    "\n",
    "        for i in range(k):\n",
    "            centroid, min_var = find_centroid(clusters[i], metric)\n",
    "            cluster_means[i] = centroid\n",
    "            cluster_variance += min_var\n",
    "\n",
    "        if cluster_variance < best_variance:\n",
    "            best_variance = cluster_variance\n",
    "            best_clustering = cluster_means.copy()\n",
    "\n",
    "        if np.all(prev_clustering == cluster_means):\n",
    "            break\n",
    "\n",
    "    cluster_means = best_clustering\n",
    "    clusters = [[cluster_means[i]] for i in range(k)]\n",
    "    for i, mail in enumerate(mails):\n",
    "        if i not in cluster_means:\n",
    "            clusters[np.argmin(metric[i, cluster_means])].append(i)\n",
    "\n",
    "    return clusters\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def davies_bouldin_index(clusters, metric):\n",
    "    k = len(clusters)\n",
    "    dispersion = []\n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        centroid, min_var = find_centroid(clusters[i], metric)\n",
    "        dispersion.append(min_var / len(clusters[i]))\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    separation = metric[np.ix_(centroids, centroids)]\n",
    "    eps = pow(10, -3)\n",
    "    separation[np.abs(separation) < eps] = 1000\n",
    "    np.fill_diagonal(separation, 1000)\n",
    "    similarity = np.array([[dispersion[i] + dispersion[j] for i in range(k)] for j in range(k)]) / separation\n",
    "\n",
    "    return np.sum(np.max(similarity, axis=1)) / k"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def find_best_clusters(mails, k_range, metric):\n",
    "    best = float(\"inf\")\n",
    "    best_clusters = []\n",
    "    quality = []\n",
    "    for i in k_range:\n",
    "        print(i)\n",
    "        clusters_ = k_means_cluster(mails, metric=metric, k=i)\n",
    "        quality.append(davies_bouldin_index(clusters_, metric))\n",
    "        if quality[-1] < best:\n",
    "            best = quality[-1]\n",
    "            best_clusters = deepcopy(clusters_)\n",
    "    return best_clusters, quality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def print_clusters(clusters):\n",
    "    for cluster in clusters:\n",
    "        print(\"######################\")\n",
    "        for ind in cluster:\n",
    "            print(emails[ind].email)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def print_clusters_parameters(clusters, q):\n",
    "    print(f\"Davies-Bouldin index value {q}\")\n",
    "    print(f\"Łącznie jest {len(clusters)} grup maili\")\n",
    "    print(f\"Największy kluster składa się z {max(len(x) for x in clusters)} maili\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b_clusters, quality = find_best_clusters(emails, range(200, 3000, 50), euclidian_metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing alfabetu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "alphabet2 = {}\n",
    "ind = 0\n",
    "for word in alphabet:\n",
    "    if count[word] > 1:\n",
    "        alphabet2[word] = ind\n",
    "        ind += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "def remove_words(mail: EMAIL):\n",
    "    new_word_dict = {}\n",
    "    for w in mail.word_dict:\n",
    "        if w in alphabet2:\n",
    "            new_word_dict[w] = mail.word_dict[w]\n",
    "    if len(new_word_dict) == 0:\n",
    "        return None\n",
    "    return EMAIL(mail.email, new_word_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7224\n",
      "6750\n"
     ]
    }
   ],
   "source": [
    "emails2 = list(filter(lambda x: x is not None, map(remove_words, emails)))\n",
    "for e in emails2:\n",
    "    indices = list(map(lambda x: alphabet2[x], e.word_dict.keys()))\n",
    "    e.index_vector = np.sort(np.array(indices))\n",
    "\n",
    "M2 = len(alphabet2)\n",
    "N2 = len(emails2)\n",
    "print(M2)\n",
    "print(N2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "idf = {}\n",
    "for w in alphabet2:\n",
    "    idf[w] = np.log(M2 / count[w])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "bag_of_words2 = create_sparse_matrix(emails2, alphabet2, M2, N2, idf_values=idf)\n",
    "bag_of_words_n2 = create_sparse_matrix(emails2, alphabet2, M2, N2, idf_values=idf, normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750\n",
      "(6750, 6750)\n",
      "(6750, 6750)\n"
     ]
    }
   ],
   "source": [
    "cosine_metric2 = cosine(bag_of_words_n2)\n",
    "euclidian_metric2 = euclidian(bag_of_words2, emails2, N2)\n",
    "print(cosine_metric2.shape)\n",
    "print(euclidian_metric2.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b_clusters = find_best_clusters(emails2, range(200, 3000, 50), euclidian_metric2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
